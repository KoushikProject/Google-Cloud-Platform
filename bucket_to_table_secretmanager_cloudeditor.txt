import pandas as pd
from google.cloud import bigquery
from google.oauth2 import service_account
from google.cloud import secretmanager
import json

#pip install pandas,pandas_gbq,gcsfs,gsspec,google-auth,google-cloud-bigquery,google-cloud-secret-manager,google-cloud-storage for storageclient
def access_secret_version(project_id, secret_id, version_id='latest'):

    # credentials = service_account.Credentials.from_service_account_file(service_account_json)

    #SecretManagerServiceClient-Used to make requests to the Secret Manager API.
    client = secretmanager.SecretManagerServiceClient()

    # Build the secret version name.
    name = f"projects/{project_id}/secrets/{secret_id}/versions/{version_id}"

    # This triggers a request to Secret Manager to retrieve the details of the specified version of the secret.
    response = client.access_secret_version(name=name)

    #This extracts the payload (the actual secret data) from the response using response.payload.data. 
    #The payload is then decoded as UTF-8 to convert it from bytes to a string.
    payload = response.payload.data.decode('UTF-8')

    return payload
def hello_gcs():
    """Triggered by a change to a Cloud Storage bucket.
    Args:
         event (dict): Event payload.
         context (google.cloud.functions.Context): Metadata for the event.
    """

    #Python client to our BigQuery instance
    # service_account_json = '/home/koushikranga200127/access_key.json'
    
    file_name = 'teacher.csv'
    project_id = 'hidden-mapper-414810'
    dataset_id = 'insertjobdataset'
    table_name = 'teacher'
    secret_id = "service_account_json_key"

    
    #Calling the function to retrieve the secret value
    secret_value = access_secret_version(project_id, secret_id)
    service_account_info = json.loads(secret_value)
    credentials = service_account.Credentials.from_service_account_info(
    service_account_info
    )
    client = bigquery.Client(project=project_id,credentials=credentials)
    #Configuration options for load jobs.
    #write_disposition - Describes whether a job should overwrite or append the existing destination table if it already exists.
    job_config = bigquery.LoadJobConfig(write_disposition = "WRITE_APPEND")

    #Reading the csv file and putting it into a dataframe.
    df_data = pd.read_csv('gs://gcp-dev-03' + '/' + file_name)
    
    
    #Load contents of a pandas DataFrame to a table.
    job = client.load_table_from_dataframe(df_data,"{}.{}.{}".format(project_id,dataset_id,table_name),job_config=job_config)
hello_gcs()